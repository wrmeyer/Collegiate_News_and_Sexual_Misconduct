{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "location = str(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11768\n"
     ]
    }
   ],
   "source": [
    "f = (location[:-8] + \"Articles\\\\articles_cleaned.csv\")\n",
    "df = pd.read_csv(f)\n",
    "df = df.to_dict(\"index\")\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11768\n"
     ]
    }
   ],
   "source": [
    "#cleaning body text\n",
    "for i in range(len(df)):\n",
    "    df[i][\"body\"] = str(df[i][\"body\"]).replace(\"\\n\", \"\").replace(\"\\r\", \"\").replace(\".\", \". \")\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(len(df)):\n",
    "    data.append(df[i][\"body\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7804116\n"
     ]
    }
   ],
   "source": [
    "def list_tokens(Data):\n",
    "    \"\"\"\n",
    "    This function will convert Articles into list of tokens(either alpha or numeric)\n",
    "    \n",
    "    arguments:\n",
    "        input_text: \"Data\" of type \"pandas Dataframe\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"list_of_tokens\" after splitting all the articles in \n",
    "                \"Processed_Content\" attribute into list of tokens.\n",
    "        \n",
    "    Example:\n",
    "    Input : This is a nice place to live. \n",
    "    Output : ['This', 'is', 'a', 'nice', 'place', 'to', 'live']\n",
    "    \"\"\"\n",
    "    list_of_articles = []\n",
    "    # For each article in Processed_Content\n",
    "    for article in Data:\n",
    "        # split each article into tokens of words either alpha or numeric (not a single punctuation)\n",
    "        tokens = re.findall(\"[\\w']+\", article) # [\\w]+ --> Checks for more than one occurrence of words which are either alpha or numeric.\n",
    "        # Appending each list of tokens for each article.\n",
    "        list_of_articles.append(tokens)  \n",
    "    # Unlisting list of list elements to only one list having all the tokens for each & every article.\n",
    "    list_of_tokens = [item for items in list_of_articles for item in items]\n",
    "    return list_of_tokens\n",
    "list_of_tokens = list_tokens(data)\n",
    "print(len(list_of_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Noise Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for item in list_of_tokens:\n",
    "    tokens.append(item.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens_filtered' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-dcf0f1182380>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens_filtered\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0men_stopwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mtokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokens_filtered' is not defined"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "for item in tokens_filtered:\n",
    "    if item not in en_stopwords:\n",
    "        tokens.append(item)\n",
    "\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing tokens that do not register as words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tokens_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tokens_filtered).to_csv('tokens_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list from csv so I don't have to refilter\n",
    "f = (location + \"\\\\tokens_filtered.csv\")\n",
    "df = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48108\n"
     ]
    }
   ],
   "source": [
    "tokens_filtered = df['0'].tolist()\n",
    "tokens_filtered = set(tokens_filtered)\n",
    "tokens_filtered = list(tokens_filtered)\n",
    "print(len(tokens_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██████▋                                                                                                              | 675/11768 [01:14<20:20,  9.09it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-2b2a0ace8ec1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens_filtered\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mcurrent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_frequency\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mnew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0marticle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mtoken_frequency\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#finding how often each token appears\n",
    "from tqdm import tqdm\n",
    "articles = data\n",
    "data = None\n",
    "\n",
    "token_frequency = {}\n",
    "\n",
    "for word in tokens_filtered:\n",
    "    token_frequency[word] = 0\n",
    "\n",
    "for i in tqdm(range(len(articles))):\n",
    "    article = articles[i]\n",
    "    for word in tokens_filtered:\n",
    "        current = int(token_frequency[word])\n",
    "        new = current + article.count(str(word))\n",
    "        token_frequency[word] = new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freq_list = []\n",
    "for word in tokens_filtered:\n",
    "    token_freq_list.append({\"token\": word, \"frequency\": token_frequency[word]})\n",
    "    \n",
    "pd.DataFrame(token_freq_list).sort_values(by=['frequency'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11021\n"
     ]
    }
   ],
   "source": [
    "from english_words import english_words_set\n",
    "\n",
    "df = []\n",
    "for item in token_freq_list:\n",
    "    if len(str(item[\"token\"])) >= 5:\n",
    "        #check if token is word\n",
    "        if item[\"token\"] in english_words_set:\n",
    "            df.append(item)\n",
    "token_freq_list = df\n",
    "print(len(token_freq_list))\n",
    "token_freq_list = pd.DataFrame(token_freq_list).sort_values(by=['frequency'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>student</td>\n",
       "      <td>39259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sexual</td>\n",
       "      <td>30497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>assault</td>\n",
       "      <td>19341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thing</td>\n",
       "      <td>17304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>people</td>\n",
       "      <td>17093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>would</td>\n",
       "      <td>16724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>campus</td>\n",
       "      <td>16584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cause</td>\n",
       "      <td>13506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>women</td>\n",
       "      <td>12479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ember</td>\n",
       "      <td>12159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>inter</td>\n",
       "      <td>11326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>event</td>\n",
       "      <td>11198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>state</td>\n",
       "      <td>11156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rough</td>\n",
       "      <td>11049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>member</td>\n",
       "      <td>10039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>issue</td>\n",
       "      <td>9975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>unity</td>\n",
       "      <td>9808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>victim</td>\n",
       "      <td>9530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>nation</td>\n",
       "      <td>9272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>think</td>\n",
       "      <td>9217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>could</td>\n",
       "      <td>9100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>resident</td>\n",
       "      <td>9085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>first</td>\n",
       "      <td>9063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>count</td>\n",
       "      <td>8945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>hough</td>\n",
       "      <td>8752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>round</td>\n",
       "      <td>8731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>right</td>\n",
       "      <td>8687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ratio</td>\n",
       "      <td>8657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>every</td>\n",
       "      <td>8652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>stand</td>\n",
       "      <td>8053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       token  frequency\n",
       "0    student      39259\n",
       "1     sexual      30497\n",
       "2    assault      19341\n",
       "3      thing      17304\n",
       "4     people      17093\n",
       "5      would      16724\n",
       "6     campus      16584\n",
       "7      cause      13506\n",
       "8      women      12479\n",
       "9      ember      12159\n",
       "10     inter      11326\n",
       "11     event      11198\n",
       "12     state      11156\n",
       "13     rough      11049\n",
       "14    member      10039\n",
       "15     issue       9975\n",
       "16     unity       9808\n",
       "17    victim       9530\n",
       "18    nation       9272\n",
       "19     think       9217\n",
       "20     could       9100\n",
       "21  resident       9085\n",
       "22     first       9063\n",
       "23     count       8945\n",
       "24     hough       8752\n",
       "25     round       8731\n",
       "26     right       8687\n",
       "27     ratio       8657\n",
       "28     every       8652\n",
       "29     stand       8053"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_freq_list.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(token_freq_list).to_csv('tokens_filtered_freq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12506\n"
     ]
    }
   ],
   "source": [
    "#Import from csv to list of dicts\n",
    "f = (location + \"\\\\tokens_filtered_freq.csv\")\n",
    "df = pd.read_csv(f)\n",
    "\n",
    "token_freq_list = []\n",
    "for item in (df.values.tolist()[1:]):\n",
    "    token_freq_list.append({\"token\": item[1], \"frequency\": item[2]})\n",
    "print(len(token_freq_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idea. Once we have a kmeans model, check each article for the makeup of terms from each cluster. \n",
    "#for example: an article can be made up of 10% cluster one 15% cluster 2 and 50% cluster 3 \n",
    "#when viewed as list of filtered tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
