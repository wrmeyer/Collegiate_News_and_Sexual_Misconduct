{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "location = str(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11768\n"
     ]
    }
   ],
   "source": [
    "f = (location[:-8] + \"Articles\\\\articles_cleaned.csv\")\n",
    "df = pd.read_csv(f)\n",
    "df = df.to_dict(\"index\")\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11768\n"
     ]
    }
   ],
   "source": [
    "#cleaning body text\n",
    "for i in range(len(df)):\n",
    "    df[i][\"body\"] = str(df[i][\"body\"]).replace(\"\\n\", \"\").replace(\"\\r\", \"\").replace(\".\", \". \")\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\wilmd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(len(df)):\n",
    "    data.append(df[i][\"body\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7804116\n"
     ]
    }
   ],
   "source": [
    "def list_tokens(Data):\n",
    "    \"\"\"\n",
    "    This function will convert Articles into list of tokens(either alpha or numeric)\n",
    "    \n",
    "    arguments:\n",
    "        input_text: \"Data\" of type \"pandas Dataframe\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"list_of_tokens\" after splitting all the articles in \n",
    "                \"Processed_Content\" attribute into list of tokens.\n",
    "        \n",
    "    Example:\n",
    "    Input : This is a nice place to live. \n",
    "    Output : ['This', 'is', 'a', 'nice', 'place', 'to', 'live']\n",
    "    \"\"\"\n",
    "    list_of_articles = []\n",
    "    # For each article in Processed_Content\n",
    "    for article in Data:\n",
    "        # split each article into tokens of words either alpha or numeric (not a single punctuation)\n",
    "        tokens = re.findall(\"[\\w']+\", article) # [\\w]+ --> Checks for more than one occurrence of words which are either alpha or numeric.\n",
    "        # Appending each list of tokens for each article.\n",
    "        list_of_articles.append(tokens)  \n",
    "    # Unlisting list of list elements to only one list having all the tokens for each & every article.\n",
    "    list_of_tokens = [item for items in list_of_articles for item in items]\n",
    "    return list_of_tokens\n",
    "list_of_tokens = list_tokens(data)\n",
    "print(len(list_of_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Noise Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for item in list_of_tokens:\n",
    "    tokens.append(item.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4179927\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "for item in tokens_filtered:\n",
    "    if item not in en_stopwords:\n",
    "        tokens.append(item)\n",
    "\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing tokens that do not register as words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")\n",
    "tokens_filtered = []\n",
    "for item in tokens:\n",
    "    if d.check(item) == True:\n",
    "        tokens_filtered.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4179927\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list from csv so I don't have to refilter\n",
    "#####CODE HERE#######\n",
    "##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tokens_filtered).to_csv('tokens_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import gensim\n",
    "model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11768\n"
     ]
    }
   ],
   "source": [
    "#finding how often each token appears\n",
    "\n",
    "#####\n",
    "#WORK HERE\n",
    "#####\n",
    "articles = data\n",
    "print(len(articles))\n",
    "token_frequency = {}\n",
    "\n",
    "for word in tokens_filtered:\n",
    "    freq_count = 0\n",
    "    for article in articles:\n",
    "        freq_count += article.count(word)\n",
    "    token_frequency[word] = freq_count\n",
    "#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"token\", \"frequency\"]\n",
    "df = pd.DataFrame.from_dict(token_frequency, orient='index', columns=cols)\n",
    "pd.DataFrame(df).to_csv('tokens_filtered_freq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idea. Once we have a kmeans model, check each article for the makeup of terms from each cluster. \n",
    "#for example: an article can be made up of 10% cluster one 15% cluster 2 and 50% cluster 3 \n",
    "#when viewed as list of filtered tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
