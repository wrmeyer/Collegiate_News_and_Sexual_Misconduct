{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import requests.exceptions\n",
    "from urllib.parse import urlsplit\n",
    "from urllib.parse import urlparse\n",
    "from collections import deque\n",
    "import re\n",
    "import lxml\n",
    "import time\n",
    "research_articles = []\n",
    "\n",
    "url = \"https://badgerherald.com/\"\n",
    "# a queue of urls to be crawled\n",
    "new_urls = deque([url])\n",
    "\n",
    "# a set of urls that we have already been processed \n",
    "processed_urls = set()\n",
    "# a set of domains inside the target website\n",
    "local_urls = set()\n",
    "# a set of domains outside the target website\n",
    "foreign_urls = set()\n",
    "# a set of broken urls\n",
    "broken_urls = set()\n",
    "\n",
    "# process urls one by one until we exhaust the queue\n",
    "x = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 5900 urls\n",
      "429\n",
      "processed 5950 urls\n",
      "430\n",
      "processed 6000 urls\n",
      "430\n",
      "processed 6050 urls\n",
      "430\n",
      "processed 6100 urls\n",
      "430\n",
      "processed 6150 urls\n",
      "432\n",
      "processed 6200 urls\n",
      "432\n",
      "processed 6250 urls\n",
      "434\n",
      "processed 6300 urls\n",
      "434\n",
      "processed 6350 urls\n",
      "435\n"
     ]
    }
   ],
   "source": [
    "while len(new_urls):\n",
    "    # move next url from the queue to the set of processed urls\n",
    "    url = new_urls.popleft()\n",
    "    processed_urls.add(url)\n",
    "    # get url's content\n",
    "    x = x+1\n",
    "    if x%50 == 0:\n",
    "        print(\"processed \" + str(x) + \" urls\")\n",
    "        print(len(research_articles))\n",
    "        time.sleep(8)\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except (requests.exceptions.MissingSchema, requests.exceptions.ConnectionError, requests.exceptions.InvalidURL, requests.exceptions.InvalidSchema):\n",
    "        # add broken urls to it's own set, then continue\n",
    "        broken_urls.add(url)\n",
    "        continue\n",
    "    time.sleep(2)\n",
    "    # extract base url to resolve relative links\n",
    "    parts = urlsplit(url)\n",
    "    base = \"{0.netloc}\".format(parts)\n",
    "    strip_base = base.replace(\"www.\", \"\")\n",
    "    base_url = \"{0.scheme}://{0.netloc}\".format(parts)\n",
    "    path = url[:url.rfind('/')+1] if '/' in parts.path else url\n",
    "\n",
    "    # create a beutiful soup for the html document\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    \n",
    "    ###########origingla code#############\n",
    "    key_words = [\"sexual assault\", \"sexual misconduct\", \"rape\", \"sexual harassment\", \"sexual abuse\"]\n",
    "    for phrase in key_words:\n",
    "        for element in soup.find_all(\"p\"):\n",
    "            if phrase in element.text and (url not in research_articles):\n",
    "                research_articles.append(url)\n",
    "        for element in soup.find_all(\"h2\"):\n",
    "            if phrase in element.text and (url not in research_articles):\n",
    "                research_articles.append(url)\n",
    "        for element in soup.find_all(\"h1\"):\n",
    "            if phrase in element.text and (url not in research_articles):\n",
    "                research_articles.append(url)\n",
    "\n",
    "    ######################################\n",
    "    for link in soup.find_all('a'):\n",
    "        # extract link url from the anchor\n",
    "        anchor = link.attrs[\"href\"] if \"href\" in link.attrs else ''\n",
    "        if anchor.startswith('/'):\n",
    "            local_link = base_url + anchor\n",
    "            local_urls.add(local_link)\n",
    "        elif strip_base in anchor:\n",
    "            local_urls.add(anchor)\n",
    "        elif not anchor.startswith('http'):\n",
    "            local_link = path + anchor\n",
    "            local_urls.add(local_link)\n",
    "        else:\n",
    "            foreign_urls.add(anchor)\n",
    "\n",
    "        for i in local_urls:\n",
    "            if not i in new_urls and not i in processed_urls:\n",
    "                if \"banter\" not in i:\n",
    "                    new_urls.append(i)\n",
    "\n",
    "print(\"finished: \" + str(len(processed_urls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('Badger_Herald_links2.json', 'w') as outfile:\n",
    "    json.dump(research_articles, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed 4950 urls\n",
    "400"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
